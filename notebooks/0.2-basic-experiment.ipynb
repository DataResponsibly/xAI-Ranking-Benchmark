{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from itertools import product\n",
    "from lightgbm import LGBMRanker\n",
    "from sklearn.utils import check_random_state\n",
    "from xai_ranking.benchmarks import (\n",
    "    human_in_the_loop_experiment,\n",
    "    hierarchical_ranking_explanation,\n",
    "    lime_experiment,\n",
    "    shap_experiment,\n",
    "    sharp_experiment,\n",
    "    rank_lime_experiment,\n",
    "    # participation_experiment,\n",
    ")\n",
    "from xai_ranking.preprocessing import (\n",
    "    preprocess_atp_data,\n",
    "    preprocess_csrank_data,\n",
    "    preprocess_higher_education_data,\n",
    "    preprocess_movers_data,\n",
    "    preprocess_synthetic_data,\n",
    ")\n",
    "from xai_ranking.datasets import (\n",
    "    fetch_atp_data,\n",
    "    fetch_csrank_data,\n",
    "    fetch_higher_education_data,\n",
    "    fetch_movers_data,\n",
    "    fetch_synthetic_data,\n",
    ")\n",
    "from xai_ranking.scorers import (\n",
    "    atp_score,\n",
    "    csrank_score,\n",
    "    higher_education_score,\n",
    "    synthetic_equal_score_3ftrs,\n",
    ")\n",
    "from xai_ranking.metrics import (\n",
    "    explanation_sensitivity,\n",
    "    outcome_sensitivity,\n",
    "    bootstrapped_explanation_consistency,\n",
    "    cross_method_explanation_consistency,\n",
    "    cross_method_outcome_consistency,\n",
    "    outcome_fidelity,\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kendalltau\n",
    "from mlresearch.utils import check_random_states, set_matplotlib_style\n",
    "from sharp.utils import scores_to_ordering\n",
    "\n",
    "RNG_SEED = 42"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set up ranker for the moving company dataset:\n",
    "X, ranks, score = preprocess_movers_data(fetch_movers_data(test=False))\n",
    "qids_train = X.index.value_counts().to_numpy()\n",
    "\n",
    "model = LGBMRanker(\n",
    "    objective=\"lambdarank\", label_gain=list(range(max(ranks) + 1)), verbose=-1\n",
    ")\n",
    "model.fit(\n",
    "    X=X,\n",
    "    y=ranks,\n",
    "    group=qids_train,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "datasets = [\n",
    "    {\n",
    "        \"name\": \"ATP\",\n",
    "        \"data\": preprocess_atp_data(fetch_atp_data()),\n",
    "        \"scorer\": atp_score,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"CSRank\",\n",
    "        \"data\": preprocess_csrank_data(fetch_csrank_data()),\n",
    "        \"scorer\": csrank_score,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Higher Education\",\n",
    "        \"data\": preprocess_higher_education_data(\n",
    "            fetch_higher_education_data(year=2020)\n",
    "        ),\n",
    "        \"scorer\": higher_education_score,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Moving Company\",\n",
    "        \"data\": preprocess_movers_data(fetch_movers_data(test=True)),\n",
    "        \"scorer\": model.predict,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Synthetic_0\",\n",
    "        \"data\": preprocess_synthetic_data(\n",
    "            fetch_synthetic_data(synth_dt_version=0, item_num=2000)\n",
    "        ),\n",
    "        \"scorer\": synthetic_equal_score_3ftrs,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Synthetic_1\",\n",
    "        \"data\": preprocess_synthetic_data(\n",
    "            fetch_synthetic_data(synth_dt_version=1, item_num=2000)\n",
    "        ),\n",
    "        \"scorer\": synthetic_equal_score_3ftrs,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Synthetic_2\",\n",
    "        \"data\": preprocess_synthetic_data(\n",
    "            fetch_synthetic_data(synth_dt_version=2, item_num=2000)\n",
    "        ),\n",
    "        \"scorer\": synthetic_equal_score_3ftrs,\n",
    "    },\n",
    "]\n",
    "xai_methods = [\n",
    "    {\n",
    "        \"iterations\": 1,\n",
    "        \"name\": \"LIME\",\n",
    "        \"experiment\": lime_experiment,\n",
    "        \"kwargs\": {\"mode\": \"regression\"},  # classification, regression\n",
    "    },\n",
    "    {\n",
    "        \"iterations\": 1,\n",
    "        \"name\": \"SHAP\",\n",
    "        \"experiment\": shap_experiment,\n",
    "        \"kwargs\": {},\n",
    "    },\n",
    "    {\n",
    "        \"iterations\": 1,\n",
    "        \"name\": \"ShaRP_RANK\",\n",
    "        \"experiment\": sharp_experiment,\n",
    "        \"kwargs\": {\n",
    "            \"qoi\": \"rank\",\n",
    "            \"verbose\": True,\n",
    "            \"sample_size\": None,\n",
    "            \"measure\": \"shapley\",\n",
    "            \"n_jobs\": -1,\n",
    "            \"replace\": True,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"iterations\": 1,\n",
    "        \"name\": \"ShaRP_SCORE\",\n",
    "        \"experiment\": sharp_experiment,\n",
    "        \"kwargs\": {\n",
    "            \"qoi\": \"rank_score\",\n",
    "            \"verbose\": True,\n",
    "            \"sample_size\": None,\n",
    "            \"measure\": \"shapley\",\n",
    "            \"n_jobs\": -1,\n",
    "            \"replace\": True,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"iterations\": 1,\n",
    "        \"name\": \"ShaRP_TOPK\",\n",
    "        \"experiment\": sharp_experiment,\n",
    "        \"kwargs\": {\n",
    "            \"qoi\": \"top_k\",\n",
    "            \"verbose\": True,\n",
    "            \"sample_size\": None,\n",
    "            \"measure\": \"shapley\",\n",
    "            \"n_jobs\": -1,\n",
    "            \"replace\": True,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"iterations\": 1,\n",
    "        \"name\": \"HRE_DT\",\n",
    "        \"experiment\": hierarchical_ranking_explanation,\n",
    "        \"kwargs\": {\"model_type\": \"DT\", \"s\": 10},  # DT, LR, OLS, PLS\n",
    "    },\n",
    "    {\n",
    "        \"iterations\": 1,\n",
    "        \"name\": \"HRE_LR\",\n",
    "        \"experiment\": hierarchical_ranking_explanation,\n",
    "        \"kwargs\": {\"model_type\": \"LR\", \"s\": 10},  # DT, LR, OLS, PLS\n",
    "    },\n",
    "    {\n",
    "        \"iterations\": 1,\n",
    "        \"name\": \"HRE_OLS\",\n",
    "        \"experiment\": hierarchical_ranking_explanation,\n",
    "        \"kwargs\": {\"model_type\": \"OLS\", \"s\": 10},  # DT, LR, OLS, PLS\n",
    "    },\n",
    "    {\n",
    "        \"iterations\": 1,\n",
    "        \"name\": \"HRE_PLS\",\n",
    "        \"experiment\": hierarchical_ranking_explanation,\n",
    "        \"kwargs\": {\"model_type\": \"PLS\", \"s\": 10},  # DT, LR, OLS, PLS\n",
    "    },\n",
    "    {\n",
    "        \"iterations\": 1,\n",
    "        \"name\": \"HIL_Shapley\",\n",
    "        \"experiment\": human_in_the_loop_experiment,\n",
    "        \"kwargs\": {\"upper_bound\": 1, \"lower_bound\": None, \"method_type\": \"shapley\"},\n",
    "    },\n",
    "    {\n",
    "        \"iterations\": 1,\n",
    "        \"name\": \"HIL_Standardized-Shapley\",\n",
    "        \"experiment\": human_in_the_loop_experiment,\n",
    "        \"kwargs\": {\n",
    "            \"upper_bound\": 1,\n",
    "            \"lower_bound\": None,\n",
    "            \"method_type\": \"standardized shapley\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"iterations\": 1,\n",
    "        \"name\": \"HIL_Rank-Shapley\",\n",
    "        \"experiment\": human_in_the_loop_experiment,\n",
    "        \"kwargs\": {\n",
    "            \"upper_bound\": 1,\n",
    "            \"lower_bound\": None,\n",
    "            \"method_type\": \"rank-relevance shapley\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"iterations\": 1,\n",
    "        \"name\": \"RankLIME\",\n",
    "        \"experiment\": rank_lime_experiment,\n",
    "        \"kwargs\": {\n",
    "            \"explanation_size\": 3,\n",
    "            \"rank_similarity_coefficient\": lambda x, y: kendalltau(x, y)[0],\n",
    "            \"individual_masking\": True,\n",
    "            \"use_entry\": 0,\n",
    "            \"use_pandas_where\": False,\n",
    "        },\n",
    "    },\n",
    "    # {\"iterations\": 1, \"name\": \"Participation\", \"experiment\": participation_experiment},\n",
    "]\n",
    "\n",
    "total_states = sum(map(lambda x: x[\"iterations\"], xai_methods)) * len(datasets)\n",
    "random_states = (x for x in check_random_states(RNG_SEED, total_states))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Uncomment to run full experiment\n",
    "\n",
    "results = {}\n",
    "for dataset in datasets:\n",
    "    results[dataset[\"name\"]] = {}\n",
    "    for xai_method in xai_methods:\n",
    "        results[dataset[\"name\"]][xai_method[\"name\"]] = []\n",
    "\n",
    "        experiment_func = xai_method[\"experiment\"]\n",
    "        score_func = dataset[\"scorer\"]\n",
    "\n",
    "        X, ranks, scores = dataset[\"data\"]\n",
    "\n",
    "        for iteration_idx in range(xai_method[\"iterations\"]):\n",
    "            random_state = next(random_states)\n",
    "            result_fname = (\n",
    "                f\"results/_contributions_{dataset['name']}_\"\n",
    "                f\"{xai_method['name']}_{iteration_idx}.csv\"\n",
    "            )\n",
    "\n",
    "            if os.path.exists(result_fname):\n",
    "                print(f\"{result_fname} exists, skipping\")\n",
    "                continue\n",
    "            if (\n",
    "                xai_method[\"name\"] in (\"HRE_LR\", \"HRE_PLS\")\n",
    "                and dataset[\"name\"] == \"Moving Company\"\n",
    "            ):\n",
    "                # dataset has binary categorical data\n",
    "                # specified methods cannot handle such data\n",
    "                continue\n",
    "            if (\n",
    "                xai_method[\"name\"].startswith(\"HIL\")\n",
    "                and dataset[\"name\"] == \"Moving Company\"\n",
    "            ):\n",
    "                # method requires weights, that are absent fot L2R\n",
    "                continue\n",
    "\n",
    "            kwargs = {} if \"kwargs\" not in xai_method else xai_method[\"kwargs\"]\n",
    "            if dataset[\"name\"] == \"Moving Company\" and xai_method[\"name\"].startswith(\n",
    "                \"ShaRP\"\n",
    "            ):\n",
    "                kwargs[\"sample_size\"] = 150\n",
    "\n",
    "            contributions = experiment_func(\n",
    "                X, score_func, random_state=random_state, **kwargs\n",
    "            )\n",
    "\n",
    "            results[dataset[\"name\"]][xai_method[\"name\"]].append(contributions)\n",
    "            result_df = pd.DataFrame(contributions, columns=X.columns, index=X.index)\n",
    "            result_df.to_csv(result_fname)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def read_results_from_files():\n",
    "    return_dict = {}\n",
    "    for cur_dataset in datasets:\n",
    "        return_dict[cur_dataset[\"name\"]] = {}\n",
    "        for cur_xai_method in xai_methods:\n",
    "            return_dict[cur_dataset[\"name\"]][cur_xai_method[\"name\"]] = []\n",
    "            for iteration in range(cur_xai_method[\"iterations\"]):\n",
    "                fname = (\n",
    "                    f\"results/_contributions_\"\n",
    "                    f\"{cur_dataset['name']}_{cur_xai_method['name']}_\"\n",
    "                    f\"{iteration}.csv\"\n",
    "                )\n",
    "                if os.path.isfile(fname):\n",
    "                    (\n",
    "                        return_dict[cur_dataset[\"name\"]][cur_xai_method[\"name\"]].append(\n",
    "                            pd.read_csv(fname, index_col=0)\n",
    "                        )\n",
    "                    )\n",
    "    return return_dict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results = read_results_from_files()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "help(explanation_sensitivity)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "methods = [method for method in results[\"ATP\"].keys() if not method.startswith(\"BATCH\")]\n",
    "\n",
    "expl_sens_res = {}\n",
    "expl_sens_sem = {}\n",
    "for method in methods:\n",
    "    expl_sens_res[method] = {}\n",
    "    expl_sens_sem[method] = {}\n",
    "    for dataset in datasets:\n",
    "        rankings = scores_to_ordering(dataset[\"scorer\"](dataset[\"data\"][0]))\n",
    "        result = explanation_sensitivity(\n",
    "            dataset[\"data\"][0],\n",
    "            results[dataset[\"name\"]][method][0],\n",
    "            rankings,\n",
    "            measure=\"jaccard\",\n",
    "            n_features=2,\n",
    "        )\n",
    "        expl_sens_res[method][dataset[\"name\"]] = result[0]\n",
    "        expl_sens_sem[method][dataset[\"name\"]] = result[1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pd.DataFrame(expl_sens_res)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outcome Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "help(outcome_sensitivity)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "methods = [method for method in results[\"ATP\"].keys() if not method.startswith(\"BATCH\")]\n",
    "\n",
    "out_sens_res = {}\n",
    "out_sens_sem = {}\n",
    "for method in methods:\n",
    "    out_sens_res[method] = {}\n",
    "    out_sens_sem[method] = {}\n",
    "    for dataset in datasets:\n",
    "        rankings = scores_to_ordering(dataset[\"scorer\"](dataset[\"data\"][0]))\n",
    "        result = outcome_sensitivity(\n",
    "            dataset[\"data\"][0],\n",
    "            dataset[\"scorer\"],\n",
    "            results[dataset[\"name\"]][method][0],\n",
    "            threshold=0.8,\n",
    "            n_neighbors=10,\n",
    "            n_tests=10,\n",
    "            std_multiplier=0.2,\n",
    "            aggregate_results=True,\n",
    "            random_state=RNG_SEED,\n",
    "        )\n",
    "        out_sens_res[method][dataset[\"name\"]] = result[0]\n",
    "        out_sens_sem[method][dataset[\"name\"]] = result[1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pd.DataFrame(out_sens_res)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation Consistency"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "help(cross_method_explanation_consistency)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "methods = [method for method in results[\"ATP\"].keys() if not method.startswith(\"BATCH\")]\n",
    "\n",
    "exp_cons_res = {}\n",
    "exp_cons_sem = {}\n",
    "for dataset in datasets:\n",
    "    exp_cons_res[dataset[\"name\"]] = pd.DataFrame(index=methods, columns=methods)\n",
    "    exp_cons_sem[dataset[\"name\"]] = pd.DataFrame(index=methods, columns=methods)\n",
    "    for method1, method2 in product(methods, methods):\n",
    "        result = cross_method_explanation_consistency(\n",
    "            results[dataset[\"name\"]][method1][0],\n",
    "            results[dataset[\"name\"]][method2][0],\n",
    "            measure=\"jaccard\",\n",
    "            n_features=2,\n",
    "        )\n",
    "\n",
    "        exp_cons_res[dataset[\"name\"]].loc[method1, method2] = result[0]\n",
    "        exp_cons_sem[dataset[\"name\"]].loc[method1, method2] = result[1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "avg_exp_cons_res = pd.DataFrame(columns=methods, index=methods).fillna(0)\n",
    "for dataset, res_ in exp_cons_res.items():\n",
    "    avg_exp_cons_res += res_\n",
    "\n",
    "avg_exp_cons_res /= len(datasets)\n",
    "avg_exp_cons_res"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "exp_cons_res[\"CSRank\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outcome Consistency"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "help(cross_method_outcome_consistency)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "methods = [method for method in results[\"ATP\"].keys() if not method.startswith(\"BATCH\")]\n",
    "\n",
    "out_cons_res = {}\n",
    "out_cons_sem = {}\n",
    "for dataset in datasets:\n",
    "    out_cons_res[dataset[\"name\"]] = pd.DataFrame(index=methods, columns=methods)\n",
    "    out_cons_sem[dataset[\"name\"]] = pd.DataFrame(index=methods, columns=methods)\n",
    "    for method1, method2 in product(methods, methods):\n",
    "        result = cross_method_outcome_consistency(\n",
    "            dataset[\"data\"][0],\n",
    "            dataset[\"scorer\"],\n",
    "            results[dataset[\"name\"]][method1][0],\n",
    "            results[dataset[\"name\"]][method2][0],\n",
    "            random_state=RNG_SEED,\n",
    "        )\n",
    "\n",
    "        out_cons_res[dataset[\"name\"]].loc[method1, method2] = result[0]\n",
    "        out_cons_sem[dataset[\"name\"]].loc[method1, method2] = result[1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "out_cons_res[\"Higher Education\"]\n",
    "\n",
    "avg_out_cons_res = pd.DataFrame(columns=methods, index=methods).fillna(0)\n",
    "for dataset, res_ in out_cons_res.items():\n",
    "    if dataset != \"Moving Company\":\n",
    "        avg_out_cons_res += res_\n",
    "\n",
    "avg_out_cons_res /= len(datasets)\n",
    "avg_out_cons_res"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapped Consistency"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "help(bootstrapped_explanation_consistency)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "methods = [method for method in results[\"ATP\"].keys() if not method.startswith(\"BATCH\")]\n",
    "\n",
    "boot_cons_res = {}\n",
    "boot_cons_sem = {}\n",
    "for method in methods:\n",
    "    boot_cons_res[method] = {}\n",
    "    boot_cons_sem[method] = {}\n",
    "    for dataset in datasets:\n",
    "        try:\n",
    "            result = bootstrapped_explanation_consistency(\n",
    "                results[dataset[\"name\"]][method][0],\n",
    "                results[dataset[\"name\"]][f\"BATCH_{method}\"],\n",
    "                measure=\"euclidean\",\n",
    "            )\n",
    "            boot_cons_res[method][dataset[\"name\"]] = result[0]\n",
    "            boot_cons_sem[method][dataset[\"name\"]] = result[1]\n",
    "        except:\n",
    "            pass"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "outcome_fidelity"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results[\"ATP\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset[\"scorer\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# FOR RANKING QOI\n",
    "methods = [method for method in results[\"ATP\"].keys() if not method.startswith(\"BATCH\")]\n",
    "dataset_names = [dataset[\"name\"] for dataset in datasets]\n",
    "\n",
    "fidelity_res = pd.DataFrame(index=dataset_names, columns=methods)\n",
    "for dataset in datasets:\n",
    "    for method in methods:\n",
    "        try:\n",
    "            target = scores_to_ordering(dataset[\"scorer\"](dataset[\"data\"][0]))\n",
    "            result = outcome_fidelity(\n",
    "                results[dataset[\"name\"]][method][0],\n",
    "                target,\n",
    "                target.mean(),\n",
    "                target_max=target.size,\n",
    "                rank=True,\n",
    "            )\n",
    "            fidelity_res.loc[dataset[\"name\"], method] = result\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "fidelity_res"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# FOR SCORE QOI\n",
    "methods = [method for method in results[\"ATP\"].keys() if not method.startswith(\"BATCH\")]\n",
    "dataset_names = [dataset[\"name\"] for dataset in datasets]\n",
    "\n",
    "fidelity_res = pd.DataFrame(index=dataset_names, columns=methods)\n",
    "for dataset in datasets:\n",
    "    for method in methods:\n",
    "        try:\n",
    "            target = dataset[\"scorer\"](dataset[\"data\"][0])\n",
    "            result = outcome_fidelity(\n",
    "                results[dataset[\"name\"]][method][0],\n",
    "                target,\n",
    "                target.mean(),\n",
    "                target_max=target.max(),\n",
    "                rank=False,\n",
    "            )\n",
    "            fidelity_res.loc[dataset[\"name\"], method] = result\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "fidelity_res"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "datasets[2][\"data\"][0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "datasets[3][\"name\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "datasets[3][\"data\"][0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from xai_ranking.metrics.old_metrics import (\n",
    "    compute_all_agreement,\n",
    "    compute_all_fidelity,\n",
    "    compute_all_sensitivity,\n",
    "    compute_all_stability,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_dataset_aggregated_summary(agg_mean, agg_sem, gap=0.3):\n",
    "    n_cols = len(agg_mean.columns)\n",
    "    x = np.arange(n_cols).astype(np.float64)\n",
    "\n",
    "    methods = agg_mean.index.unique()\n",
    "    bars = len(methods)\n",
    "    bar_width = (1 - gap) / bars\n",
    "    x -= (bars - 1) * bar_width / 2\n",
    "\n",
    "    for method in methods:\n",
    "        plt.errorbar(\n",
    "            x,\n",
    "            agg_mean.loc[method],\n",
    "            yerr=agg_sem.loc[method],\n",
    "            marker=\"o\",\n",
    "            label=method,\n",
    "            linestyle=\"None\",\n",
    "        )\n",
    "        x += bar_width\n",
    "    plt.legend()\n",
    "    plt.xticks(np.arange(n_cols), agg_mean.columns, rotation=45)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "set_matplotlib_style(font_size=12, **{\"font.family\": [\"Nimbus Roman\"]})\n",
    "aggregated_summary, aggregated_error = compute_all_stability(results, axis=0)\n",
    "for dataset in aggregated_summary:\n",
    "    plot_dataset_aggregated_summary(\n",
    "        aggregated_summary[dataset], aggregated_error[dataset]\n",
    "    )\n",
    "    plt.title(dataset)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agreement"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "agreement_results = compute_all_agreement(results, n_features=3)\n",
    "agreement_results[\"ATP\"][\"kendall\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "agreement_results[\"ATP\"][\"jaccard\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "agreement_results[\"CSRank\"][\"kendall\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "agreement_results[\"CSRank\"][\"jaccard\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "agreement_results[\"Higher Education\"][\"kendall\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sensitivity_results = compute_all_sensitivity(\n",
    "    original_data=datasets, results=results, n_neighbors=10\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pd.DataFrame(sensitivity_results[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fidelity_results = compute_all_fidelity(\n",
    "    original_data=datasets, results=results, random_state=RNG_SEED\n",
    ")\n",
    "pd.DataFrame(fidelity_results[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pd.DataFrame(fidelity_results[0]).mean(1)  # .drop(columns=\"Moving Company\").mean(1)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
