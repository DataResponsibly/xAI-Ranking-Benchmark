{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from lightgbm import LGBMRanker\n",
    "from xai_ranking.benchmarks import (\n",
    "    human_in_the_loop_experiment,\n",
    "    human_in_the_loop_batch_experiment,\n",
    "    hierarchical_ranking_explanation,\n",
    "    hierarchical_ranking_batch_explanation,\n",
    "    lime_experiment,\n",
    "    lime_batch_experiment,\n",
    "    shap_experiment,\n",
    "    shap_batch_experiment,\n",
    "    sharp_experiment,\n",
    "    sharp_batch_experiment,\n",
    "    # participation_experiment,\n",
    ")\n",
    "from xai_ranking.preprocessing import (\n",
    "    preprocess_atp_data,\n",
    "    preprocess_csrank_data,\n",
    "    preprocess_higher_education_data,\n",
    "    preprocess_movers_data,\n",
    ")\n",
    "from xai_ranking.datasets import (\n",
    "    fetch_atp_data,\n",
    "    fetch_csrank_data,\n",
    "    fetch_higher_education_data,\n",
    "    fetch_movers_data,\n",
    ")\n",
    "from xai_ranking.scorers import (\n",
    "    atp_score,\n",
    "    csrank_score,\n",
    "    higher_education_score,\n",
    ")\n",
    "from xai_ranking.metrics import compute_all_stability, compute_all_agreement\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mlresearch.utils import check_random_states, set_matplotlib_style\n",
    "\n",
    "RNG_SEED = 42"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set up ranker for the moving company dataset:\n",
    "X, ranks, score = preprocess_movers_data(fetch_movers_data(test=False))\n",
    "qids_train = X.index.value_counts().to_numpy()\n",
    "\n",
    "model = LGBMRanker(\n",
    "    objective=\"lambdarank\", label_gain=list(range(max(ranks) + 1)), verbose=-1\n",
    ")\n",
    "model.fit(\n",
    "    X=X,\n",
    "    y=ranks,\n",
    "    group=qids_train,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "datasets = [\n",
    "    {\n",
    "        \"name\": \"ATP\",\n",
    "        \"data\": preprocess_atp_data(fetch_atp_data()),\n",
    "        \"scorer\": atp_score,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"CSRank\",\n",
    "        \"data\": preprocess_csrank_data(fetch_csrank_data()),\n",
    "        \"scorer\": csrank_score,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Higher Education\",\n",
    "        \"data\": preprocess_higher_education_data(\n",
    "            fetch_higher_education_data(year=2020)\n",
    "        ),\n",
    "        \"scorer\": higher_education_score,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Moving Company\",\n",
    "        \"data\": preprocess_movers_data(fetch_movers_data(test=True)),\n",
    "        \"scorer\": model.predict,\n",
    "    },\n",
    "]\n",
    "xai_methods = [\n",
    "    {\n",
    "        \"iterations\": 1,\n",
    "        \"name\": \"LIME\",\n",
    "        \"experiment\": lime_experiment,\n",
    "        \"kwargs\": {\"mode\": \"regression\"}  # classification, regression\n",
    "    },\n",
    "    {\n",
    "        \"iterations\": 10,\n",
    "        \"name\": \"BATCH_LIME\",\n",
    "        \"experiment\": lime_batch_experiment,\n",
    "        \"kwargs\": {\"mode\": \"regression\"}  # classification, regression\n",
    "    },\n",
    "    {\n",
    "        \"iterations\": 1,\n",
    "        \"name\": \"SHAP\",\n",
    "        \"experiment\": shap_experiment,\n",
    "        \"kwargs\": {}\n",
    "    },\n",
    "    {\n",
    "        \"iterations\": 10,\n",
    "        \"name\": \"BATCH_SHAP\",\n",
    "        \"experiment\": shap_batch_experiment,\n",
    "        \"kwargs\": {}\n",
    "    },\n",
    "    {\n",
    "        \"iterations\": 1,\n",
    "        \"name\": \"ShaRP\",\n",
    "        \"experiment\": sharp_experiment,\n",
    "        \"kwargs\": {\n",
    "            \"verbose\": True,\n",
    "            \"sample_size\": 150,\n",
    "            \"measure\": \"shapley\",\n",
    "            \"n_jobs\": -1\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"iterations\": 10,\n",
    "        \"name\": \"BATCH_ShaRP\",\n",
    "        \"experiment\": sharp_batch_experiment,\n",
    "        \"kwargs\": {\n",
    "            \"verbose\": True,\n",
    "            \"sample_size\": 150,\n",
    "            \"measure\": \"shapley\",\n",
    "            \"n_jobs\": -1\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"iterations\": 1,\n",
    "        \"name\": \"HRE\",\n",
    "        \"experiment\": hierarchical_ranking_explanation,\n",
    "        \"kwargs\": {\n",
    "            \"model_type\": \"OLS\",  # DT, LR, OLS, PLS\n",
    "            \"s\": 5\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"iterations\": 10,\n",
    "        \"name\": \"BATCH_HRE\",\n",
    "        \"experiment\": hierarchical_ranking_batch_explanation,\n",
    "        \"kwargs\": {\n",
    "            \"model_type\": \"OLS\",  # DT, LR, OLS, PLS\n",
    "            \"s\": 5\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"iterations\": 1,\n",
    "        \"name\": \"HIL\",\n",
    "        \"experiment\": human_in_the_loop_experiment,\n",
    "        \"kwargs\": {\"upper_bound\": 1, \"lower_bound\": None}\n",
    "    },\n",
    "    {\n",
    "        \"iterations\": 10,\n",
    "        \"name\": \"BATCH_HIL\",\n",
    "        \"experiment\": human_in_the_loop_batch_experiment,\n",
    "        \"kwargs\": {\"upper_bound\": 1, \"lower_bound\": None}\n",
    "    },\n",
    "    # {\"iterations\": 1, \"name\": \"Participation\", \"experiment\": participation_experiment},\n",
    "]\n",
    "\n",
    "total_states = sum(map(lambda x: x[\"iterations\"], xai_methods)) * len(datasets)\n",
    "random_states = (x for x in check_random_states(RNG_SEED, total_states))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results = {}\n",
    "for dataset in datasets:\n",
    "    results[dataset[\"name\"]] = {}\n",
    "    for xai_method in xai_methods:\n",
    "        results[dataset[\"name\"]][xai_method[\"name\"]] = []\n",
    "\n",
    "        experiment_func = xai_method[\"experiment\"]\n",
    "        preprocess_func = dataset[\"preprocess\"]\n",
    "        score_func = dataset[\"scorer\"]\n",
    "\n",
    "        X, ranks, scores = preprocess_func(dataset[\"data\"])\n",
    "\n",
    "        for iteration_idx in range(xai_method[\"iterations\"]):\n",
    "            random_state = next(random_states)\n",
    "            if \"done\" in dataset and dataset[\"done\"]:\n",
    "                continue\n",
    "\n",
    "            kwargs = {} if \"kwargs\" not in xai_method else xai_method[\"kwargs\"]\n",
    "            contributions = experiment_func(X, score_func, random_state=random_state, **kwargs)\n",
    "\n",
    "            results[dataset[\"name\"]][xai_method[\"name\"]].append(contributions)\n",
    "            result_df = pd.DataFrame(contributions, columns=X.columns, index=X.index)\n",
    "            result_df.to_csv(\n",
    "                f\"results/_contributions_{dataset['name']}_{xai_method['name']}_{iteration_idx}.csv\"\n",
    "            )\n",
    "        # with open(f\"_contributions_{dataset['name']}_{xai_method['name']}.npy\", \"wb\") as f:\n",
    "        #     np.save(f, contributions)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def read_results_from_files():\n",
    "    return_dict = {}\n",
    "    for cur_dataset in datasets:\n",
    "        return_dict[cur_dataset[\"name\"]] = {}\n",
    "        for cur_xai_method in xai_methods:\n",
    "            return_dict[cur_dataset[\"name\"]][cur_xai_method[\"name\"]] = []\n",
    "            for iteration in range(cur_xai_method[\"iterations\"]):\n",
    "                fname = (\n",
    "                    f\"partial-results/_contributions_\"\n",
    "                    f\"{cur_dataset['name']}_{cur_xai_method['name']}_\"\n",
    "                    f\"{iteration}.csv\"\n",
    "                )\n",
    "                if os.path.isfile(fname):\n",
    "                    (\n",
    "                        return_dict[cur_dataset[\"name\"]][cur_xai_method[\"name\"]].append(\n",
    "                            pd.read_csv(fname, index_col=0)\n",
    "                        )\n",
    "                    )\n",
    "    return return_dict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results = read_results_from_files()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stability"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "aggregated_summary, aggregated_error = compute_all_stability(results, axis=None)\n",
    "pd.DataFrame(aggregated_summary)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_dataset_aggregated_summary(agg_mean, agg_sem, gap=0.3):\n",
    "    n_cols = len(agg_mean.columns)\n",
    "    x = np.arange(n_cols).astype(np.float64)\n",
    "\n",
    "    methods = agg_mean.index.unique()\n",
    "    bars = len(methods)\n",
    "    bar_width = (1 - gap) / bars\n",
    "    x -= (bars - 1) * bar_width / 2\n",
    "\n",
    "    for method in methods:\n",
    "        plt.errorbar(\n",
    "            x,\n",
    "            agg_mean.loc[method],\n",
    "            yerr=agg_sem.loc[method],\n",
    "            marker=\"o\",\n",
    "            label=method,\n",
    "            linestyle=\"None\",\n",
    "        )\n",
    "        x += bar_width\n",
    "    plt.legend()\n",
    "    plt.xticks(np.arange(n_cols), agg_mean.columns, rotation=45)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "set_matplotlib_style(font_size=12, **{\"font.family\": [\"Nimbus Roman\"]})\n",
    "aggregated_summary, aggregated_error = compute_all_stability(results, axis=0)\n",
    "for dataset in aggregated_summary:\n",
    "    plot_dataset_aggregated_summary(\n",
    "        aggregated_summary[dataset], aggregated_error[dataset]\n",
    "    )\n",
    "    plt.title(dataset)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agreement"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "agreement_results = compute_all_agreement(results, n_features=3)\n",
    "agreement_results[\"ATP\"][\"kendall\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "agreement_results[\"ATP\"][\"jaccard\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "agreement_results[\"ATP\"][\"kendall\"].mean(), agreement_results[\"ATP\"][\n",
    "    \"jaccard\"\n",
    "].mean()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
